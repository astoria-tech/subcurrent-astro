{
  "title": "Talking to LLMs locally with Ollama with Node.js",
  "link": "https://www.youtube.com/watch?v=JOrX3V-g5ZI",
  "pubDate": "2024-09-25T19:00:19.000Z",
  "snippet": "Live streamed on September 11, 2024. \n\nRunning LLMs locally is amazing, and Ollama makes it possible. Even better, Ollama isn't just a CLIâ€”it also comes with a local REST API server.\n\nIn the video, look at running Llama 3.1 and Tiny Llama via CLI. Then we get into calling LLMs from a Node.js script.\n\nGet Ollama here: https://ollama.com",
  "author": "meremortaldev",
  "feedSource": "https://www.youtube.com/feeds/videos.xml?channel_id=UCjkzTpA7V8AfJS9UKLKFZxA",
  "lastFetched": "2025-02-23T00:04:34.408Z"
}